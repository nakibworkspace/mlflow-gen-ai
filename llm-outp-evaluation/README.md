## Goal
Extend your tracing pipeline to evaluate LLM outputs automatically and log evaluation as child spans in MLflow.

## Objectives
- what evaluation means in GenAI
- how to score LLM outputs
- how to log evaluator spans
- how production LLM monitoring works

## Core Concepts
1. Generation â‰  Evaluation
```
Generation = model produces text
Evaluation = system judges text quality
```
### Production pipelines:
```
Prompt â†’ LLM â†’ Evaluators â†’ Dashboard
```

## Evaluation Types

There are 3 major classes:

## Evaluation Metrics Summary

| Evaluation Type | Example Metric | Description |
| :--- | :--- | :--- |
| **Rule-based** | Length Score / Regex | Measures deterministic constraints and formatting. |
| **Embedding-based** | Semantic Similarity | Uses vector distance (e.g., Cosine) to check meaning. |
| **Model-based** | LLM-as-a-Judge | Uses a high-reasoning model to score quality/nuance. |

In this lab we implement:
Rule + Semantic evaluation

### Why Evaluations Are Logged as Spans
Because evaluation is a step of execution, not metadata.

Trace tree:
```
Generation span
 â”œâ”€â”€ Length evaluator
 â””â”€â”€ Similarity evaluator
```

This gives debugging visibility.

### Project Structure
```
llm-outp-evaluation/
â”‚
â”œâ”€â”€ run_eval.py
â”œâ”€â”€ dataset.txt
â””â”€â”€ reference.txt
```

## Run Lab

Start UI:
```bash
mlflow ui
```

### Run script:
```bash
python3 run_eval.py
```

Open:
```bash
http://127.0.0.1:5000
```

ðŸ”Ž What You Should See in UI

Trace tree:
```
Run
 â””â”€â”€ generation
      â”œâ”€â”€ length_eval
      â””â”€â”€ semantic_eval
```

Click each span to inspect:
1. inputs
2. outputs
3. scores